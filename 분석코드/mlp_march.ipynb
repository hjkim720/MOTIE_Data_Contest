{"cells":[{"cell_type":"code","execution_count":9,"metadata":{"id":"rX_La3qvOmum","executionInfo":{"status":"ok","timestamp":1752414980033,"user_tz":-540,"elapsed":3,"user":{"displayName":"ê¹€íš¨ì¤€","userId":"15786858277088276326"}}},"outputs":[],"source":["# âœ… 0. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ë° ì‹œë“œ ê³ ì •\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.preprocessing import StandardScaler\n","from itertools import product\n","\n","torch.manual_seed(42)\n","np.random.seed(42)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"R-5eRBekY6gj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1752414981548,"user_tz":-540,"elapsed":1510,"user":{"displayName":"ê¹€íš¨ì¤€","userId":"15786858277088276326"}},"outputId":"f7781ec8-f8e7-407c-aac9-eb84cbf81897"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬\n","import pandas as pd, numpy as np, xgboost as xgb\n","from tqdm import tqdm\n","from google.colab import drive\n","drive.mount('/content/drive')\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 2. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ (ê´€ì¸¡ + ì˜ˆì¸¡)\n","obs  = pd.read_csv('drive/MyDrive/ë°ì´í„°_ë¶„ì„ê³¼ì œ_7_ê¸°ìƒê´€ì¸¡ë°ì´í„°_2401_2503.csv')\n","pred = pd.read_csv('drive/MyDrive/ë°ì´í„°_ë¶„ì„ê³¼ì œ_7_ê¸°ìƒì˜ˆì¸¡ë°ì´í„°_2401_2503.csv')\n","obs.columns  = ['datetime', 'humidity_obs', 'temp_obs', 'pressure_obs']\n","pred.columns = ['datetime', 'solar_rad_pred', 'humidity_pred', 'absolute_humidity_pred', 'temp_pred', 'pressure_pred']\n","\n","# ë‹¨ìœ„ í†µì¼ (pressure: mmHg â†’ hPa)\n","obs['pressure_obs'] = obs['pressure_obs'] * 1.33322\n","obs['datetime']     = pd.to_datetime(obs['datetime'])\n","pred['datetime']    = pd.to_datetime(pred['datetime'])\n","\n","df = pd.merge(obs, pred, on='datetime')\n","\n","# ì˜¤ì°¨ ë³€ìˆ˜ ìƒì„±\n","df['temp_error']     = df['temp_pred']     - df['temp_obs']\n","df['humidity_error'] = df['humidity_pred'] - df['humidity_obs']\n","df['pressure_error'] = df['pressure_pred'] - df['pressure_obs']\n","\n","# ì‹œê°„ íŒŒìƒë³€ìˆ˜\n","df['hour']  = df['datetime'].dt.hour\n","df['month'] = df['datetime'].dt.month\n","df['day']   = df['datetime'].dt.day\n","df['year']  = df['datetime'].dt.year\n","\n","df['hour_sin']  = np.sin(2 * np.pi * df['hour'] / 24)\n","df['hour_cos']  = np.cos(2 * np.pi * df['hour'] / 24)\n","df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n","df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n","\n","#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 3. ì´ìƒì¹˜ ì œê±°\n","def get_outlier_indices(series):\n","    Q1 = series.quantile(0.25)\n","    Q3 = series.quantile(0.75)\n","    IQR = Q3 - Q1\n","    lower = Q1 - 1.5 * IQR\n","    upper = Q3 + 1.5 * IQR\n","    return series[(series < lower) | (series > upper)].index\n","\n","out_idx = set(get_outlier_indices(df['pressure_error']))\n","out_idx |= set(get_outlier_indices(df['temp_error']))\n","out_idx |= set(get_outlier_indices(df['humidity_error']))\n","\n","drop_outliers = df.drop(index=out_idx).reset_index(drop=True)\n","ori_data = df.copy()\n","\n","\n","# ë°ì´í„° ë¶„í• \n","train_df = drop_outliers[\n","    (drop_outliers[\"datetime\"] >= \"2024-01-01\") & (drop_outliers[\"datetime\"] < \"2025-01-01\")\n","]\n","val_df = ori_data[\n","    (ori_data[\"datetime\"] >= \"2025-01-01\") & (ori_data[\"datetime\"] < \"2025-03-01\")\n","]\n","test_df = ori_data[\n","    (ori_data[\"datetime\"] >= \"2025-03-01\") & (ori_data[\"datetime\"] < \"2025-04-01\")\n","]\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"_xxsgYJYY6ls","executionInfo":{"status":"ok","timestamp":1752414981552,"user_tz":-540,"elapsed":6,"user":{"displayName":"ê¹€íš¨ì¤€","userId":"15786858277088276326"}}},"outputs":[],"source":["# ê³µí†µ ì…ë ¥ ì»¬ëŸ¼\n","input_cols = [\n","    \"temp_pred\", \"humidity_pred\", \"pressure_pred\",\n","    \"absolute_humidity_pred\", \"solar_rad_pred\",\n","    \"hour_sin\", \"hour_cos\", \"month_sin\", \"month_cos\",\n","    \"month\", \"day\", \"hour\",\n","]\n","\n","# âœ… ê³µí†µ Dataset ì •ì˜\n","class ForecastDataset(Dataset):\n","    def __init__(self, X, y):\n","        self.X = torch.tensor(X, dtype=torch.float32)\n","        self.y = torch.tensor(y, dtype=torch.float32)\n","    def __len__(self):\n","        return len(self.X)\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n","\n","# âœ… ê³µí†µ ëª¨ë¸ ì •ì˜\n","class ForecastMLP(nn.Module):\n","    def __init__(self, input_dim, hidden_dim=128, output_dim=1):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, hidden_dim // 2),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim // 2, output_dim)\n","        )\n","    def forward(self, x):\n","        return self.net(x)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"4_Tycjn7Y6o7","executionInfo":{"status":"ok","timestamp":1752414981556,"user_tz":-540,"elapsed":2,"user":{"displayName":"ê¹€íš¨ì¤€","userId":"15786858277088276326"}}},"outputs":[],"source":["# âœ… ì†ì‹¤ ë° í‰ê°€ í•¨ìˆ˜\n","class RMSE_MAE_Loss(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","    def forward(self, pred, target):\n","        rmse = torch.sqrt(torch.mean((pred - target) ** 2))\n","        mae = torch.mean(torch.abs(pred - target))\n","        return (rmse + mae) / 2\n","\n","def evaluate_single_target(y_true, y_pred, scaler_y):\n","    y_true = scaler_y.inverse_transform(y_true)\n","    y_pred = scaler_y.inverse_transform(y_pred)\n","    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n","    mae = np.mean(np.abs(y_true - y_pred))\n","    return (rmse + mae) / 2\n"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"C4lzwBq6Y6rr","executionInfo":{"status":"ok","timestamp":1752414981565,"user_tz":-540,"elapsed":8,"user":{"displayName":"ê¹€íš¨ì¤€","userId":"15786858277088276326"}}},"outputs":[],"source":["def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler,\n","                scaler_y=None, num_epochs=3000, patience=50, verbose=True):\n","    best_val_score = float(\"inf\")\n","    best_model_state = None\n","    epochs_no_improve = 0\n","\n","    train_losses = []\n","    val_scores = []\n","    lrs = []\n","\n","    for epoch in range(1, num_epochs + 1):\n","        model.train()\n","        running_loss = 0\n","        for xb, yb in train_loader:\n","            pred = model(xb)\n","            loss = criterion(pred, yb)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","\n","        avg_loss = running_loss / len(train_loader)\n","        train_losses.append(avg_loss)\n","\n","        # ê²€ì¦\n","        model.eval()\n","        val_preds, val_trues = [], []\n","        with torch.no_grad():\n","            for xb, yb in val_loader:\n","                pred = model(xb)\n","                val_preds.append(pred.numpy())\n","                val_trues.append(yb.numpy())\n","\n","        val_score = evaluate_single_target(np.vstack(val_trues), np.vstack(val_preds), scaler_y)\n","        val_scores.append(val_score)\n","        lrs.append(optimizer.param_groups[0][\"lr\"])\n","\n","        scheduler.step(val_score)\n","\n","        if verbose and (epoch % 10 == 0 or epoch == 1):\n","            print(f\"[Epoch {epoch:04d}] Train Loss: {avg_loss:.6f} | Val sAError: {val_score:.6f}\")\n","\n","        if val_score < best_val_score:\n","            best_val_score = val_score\n","            best_model_state = model.state_dict()\n","            epochs_no_improve = 0\n","        else:\n","            epochs_no_improve += 1\n","            if epochs_no_improve >= patience:\n","                if verbose:\n","                    print(f\"\\nğŸ›‘ Early stopping at epoch {epoch}\")\n","                break\n","\n","    return best_model_state, train_losses, val_scores, lrs\n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"5OmqwOTkY6uT","executionInfo":{"status":"ok","timestamp":1752414981568,"user_tz":-540,"elapsed":2,"user":{"displayName":"ê¹€íš¨ì¤€","userId":"15786858277088276326"}}},"outputs":[],"source":["def run_forecast_pipeline(target_col, lr, sched_patience, factor,\n","                          hidden_dim=128, train_patience=50, batch_size=64):\n","    print(f\"\\nğŸš€ Target: {target_col}\")\n","\n","    # 1. ì…ë ¥/ì¶œë ¥ ì •ì˜\n","    X_train = train_df[input_cols].copy()\n","    X_val   = val_df[input_cols].copy()\n","    X_test  = test_df[input_cols].copy()\n","\n","    y_train = train_df[[target_col]].copy()\n","    y_val   = val_df[[target_col]].copy()\n","    y_test  = test_df[[target_col]].copy()\n","\n","    # 2. ì •ê·œí™”\n","    scaler_X = StandardScaler()\n","    scaler_y = StandardScaler()\n","\n","    X_train = scaler_X.fit_transform(X_train)\n","    X_val   = scaler_X.transform(X_val)\n","    X_test  = scaler_X.transform(X_test)\n","\n","    y_train = scaler_y.fit_transform(y_train)\n","    y_val   = scaler_y.transform(y_val)\n","    y_test  = scaler_y.transform(y_test)\n","\n","    # 3. Dataloader\n","    train_loader = DataLoader(ForecastDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n","    val_loader   = DataLoader(ForecastDataset(X_val, y_val), batch_size=batch_size)\n","    test_loader  = DataLoader(ForecastDataset(X_test, y_test), batch_size=batch_size)\n","\n","    # 4. ëª¨ë¸ êµ¬ì„±\n","    model = ForecastMLP(input_dim=len(input_cols), hidden_dim=hidden_dim, output_dim=1)\n","    criterion = RMSE_MAE_Loss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n","        patience=sched_patience, factor=factor\n","    )\n","\n","    # 5. í•™ìŠµ\n","    best_model_state, _, _, _ = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler,\n","                                   scaler_y=scaler_y, patience=train_patience)\n","    model.load_state_dict(best_model_state)\n","    model.eval()\n","\n","    # 6. ì˜ˆì¸¡\n","    test_preds = []\n","    with torch.no_grad():\n","        for xb, _ in test_loader:\n","            pred = model(xb)\n","            test_preds.append(pred.numpy())\n","\n","    test_preds = np.vstack(test_preds)\n","    preds = scaler_y.inverse_transform(test_preds).flatten()\n","    trues = scaler_y.inverse_transform(y_test).flatten()\n","\n","    var_name = target_col.replace(\"_obs\", \"\")\n","\n","    # 7. ì˜ˆì¸¡ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n","    result_df = pd.DataFrame({\n","        \"datetime\": test_df[\"datetime\"].values,\n","        f\"{var_name}_pred\": preds,\n","        f\"{var_name}_true\": trues\n","    })\n","\n","    return result_df\n"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CkeYbbygY6w3","outputId":"bea03204-b575-4568-c793-44e89008507c","executionInfo":{"status":"ok","timestamp":1752415081072,"user_tz":-540,"elapsed":99503,"user":{"displayName":"ê¹€íš¨ì¤€","userId":"15786858277088276326"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","ğŸš€ Target: temp_obs\n","[Epoch 0001] Train Loss: 0.713080 | Val sAError: 5.717346\n","[Epoch 0010] Train Loss: 0.110478 | Val sAError: 1.655318\n","[Epoch 0020] Train Loss: 0.101423 | Val sAError: 1.561035\n","[Epoch 0030] Train Loss: 0.096939 | Val sAError: 1.538366\n","[Epoch 0040] Train Loss: 0.093713 | Val sAError: 1.545891\n","[Epoch 0050] Train Loss: 0.091388 | Val sAError: 1.529574\n","[Epoch 0060] Train Loss: 0.089148 | Val sAError: 1.589146\n","[Epoch 0070] Train Loss: 0.087511 | Val sAError: 1.584650\n","[Epoch 0080] Train Loss: 0.086938 | Val sAError: 1.590476\n","[Epoch 0090] Train Loss: 0.086521 | Val sAError: 1.597244\n","\n","ğŸ›‘ Early stopping at epoch 92\n","\n","ğŸš€ Target: humidity_obs\n","[Epoch 0001] Train Loss: 0.899392 | Val sAError: 16.591209\n","[Epoch 0010] Train Loss: 0.616000 | Val sAError: 12.162769\n","[Epoch 0020] Train Loss: 0.464740 | Val sAError: 9.980891\n","[Epoch 0030] Train Loss: 0.436161 | Val sAError: 9.649940\n","[Epoch 0040] Train Loss: 0.424715 | Val sAError: 9.590842\n","[Epoch 0050] Train Loss: 0.417444 | Val sAError: 9.624107\n","[Epoch 0060] Train Loss: 0.412461 | Val sAError: 9.641796\n","[Epoch 0070] Train Loss: 0.408906 | Val sAError: 9.701305\n","[Epoch 0080] Train Loss: 0.405939 | Val sAError: 9.732091\n","[Epoch 0090] Train Loss: 0.403420 | Val sAError: 9.763916\n","\n","ğŸ›‘ Early stopping at epoch 92\n","\n","ğŸš€ Target: pressure_obs\n","[Epoch 0001] Train Loss: 0.801948 | Val sAError: 4.742311\n","[Epoch 0010] Train Loss: 0.107227 | Val sAError: 1.081176\n","[Epoch 0020] Train Loss: 0.082981 | Val sAError: 0.963938\n","[Epoch 0030] Train Loss: 0.078082 | Val sAError: 0.936818\n","[Epoch 0040] Train Loss: 0.075386 | Val sAError: 0.927028\n","[Epoch 0050] Train Loss: 0.073520 | Val sAError: 0.891223\n","[Epoch 0060] Train Loss: 0.072306 | Val sAError: 0.901656\n","[Epoch 0070] Train Loss: 0.071012 | Val sAError: 0.897695\n","[Epoch 0080] Train Loss: 0.070113 | Val sAError: 0.909446\n","[Epoch 0090] Train Loss: 0.069005 | Val sAError: 0.905877\n","[Epoch 0100] Train Loss: 0.068661 | Val sAError: 0.909560\n","[Epoch 0110] Train Loss: 0.068337 | Val sAError: 0.894527\n","\n","ğŸ›‘ Early stopping at epoch 117\n","\n","ğŸ“Š sAError ê²°ê³¼:\n"," - temp     : 1.421862\n"," - humidity : 9.187578\n"," - pressure : 0.559507\n","\n","âœ… ì´í•© sAError = 3.579106\n","\n","ğŸ“ forecast_march2025.csv ì €ì¥ ì™„ë£Œ!\n"]}],"source":["# ê° ë³€ìˆ˜ë³„ ëª¨ë¸ ì‹¤í–‰\n","df_temp = run_forecast_pipeline(\"temp_obs\",     lr=0.0001, sched_patience=20, factor=0.3)\n","df_humi = run_forecast_pipeline(\"humidity_obs\", lr=0.00001, sched_patience=20, factor=0.9)\n","df_pres = run_forecast_pipeline(\"pressure_obs\", lr=0.00003, sched_patience=20, factor=0.4)\n","\n","# datetime ê¸°ì¤€ ë³‘í•©\n","merged_df = df_temp.merge(df_humi, on=\"datetime\").merge(df_pres, on=\"datetime\")\n","\n","# sAError ê³„ì‚°\n","def compute_sAError(true, pred):\n","    rmse = np.sqrt(np.mean((true - pred) ** 2))\n","    mae = np.mean(np.abs(true - pred))\n","    return (rmse + mae) / 2\n","merged_df['pressure_true']/=1.33322\n","merged_df[\"pressure_pred\"]/=1.33322\n","sa_temp = compute_sAError(merged_df[\"temp_true\"], merged_df[\"temp_pred\"])\n","sa_humi = compute_sAError(merged_df[\"humidity_true\"], merged_df[\"humidity_pred\"])\n","sa_pres = compute_sAError(merged_df[\"pressure_true\"], merged_df[\"pressure_pred\"])\n","\n","print(\"\\nğŸ“Š sAError ê²°ê³¼:\")\n","print(f\" - temp     : {sa_temp:.6f}\")\n","print(f\" - humidity : {sa_humi:.6f}\")\n","print(f\" - pressure : {sa_pres:.6f}\")\n","\n","# ì´í•© sAError (ê°€ì¤‘ì¹˜ ì ìš©)\n","weights = [0.5, 0.3, 0.2]\n","total_sAError = weights[0]*sa_temp + weights[1]*sa_humi + weights[2]*sa_pres\n","print(f\"\\nâœ… ì´í•© sAError = {total_sAError:.6f}\")\n","\n","# âœ… ìµœì¢… ê²°ê³¼ ì €ì¥\n","final_df = merged_df[[\"datetime\", \"humidity_pred\", \"temp_pred\", \"pressure_pred\"]]\n","final_df.columns = [\"datetime\", \"humidity\", \"temp\", \"pressure\"]\n","final_df.to_csv(\"/content/drive/MyDrive/forecast_march_full.csv\", index=False)\n","print(\"\\nğŸ“ forecast_march2025.csv ì €ì¥ ì™„ë£Œ!\")\n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"Qas1XaulfkbF","executionInfo":{"status":"ok","timestamp":1752415081092,"user_tz":-540,"elapsed":6,"user":{"displayName":"ê¹€íš¨ì¤€","userId":"15786858277088276326"}}},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}